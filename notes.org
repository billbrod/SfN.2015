#+LATEX_CLASS: article
#+DATE: 2015-04-27
#+TITLE: Penalty Shot SfN 2015
#+OPTIONS: toc:nil author:nil date:nil ^:{}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{titling}
#+LATEX_HEADER: \setlength{\droptitle}{-1.5cm}
#+LATEX_HEADER: \posttitle{\par\end{center}\vspace{-1cm}}
#+LATEX_HEADER: \setlength{\parindent}{0cm}
#+LATEX_HEADER: \usepackage[margin=2cm]{geometry} \usepackage{svg}
#+KEYWORDS: [+PenaltyKik, +PenaltyShot, +Abstract, +Presentation, +SfN2015]

* Abstract
  #+AUTHOR: William F. Broderick, R. McKell Carter, Mariano Tepper, Jean-François Gariépy, Michael L. Platt, Guillermo Sapiro, Scott A. Huettel

  Theme-subtheme-subject: F.01.p.

  Cognition and Behavior -- Human Cognition and Behavior -- Decision making and reasoning

** A multi-variate pattern analysis investigation of strategic thinking and deception in a dynamic, competitive game.

   Successful human interactions depend upon the interpretation of an
   agent's behaviors and intentions in order to support strategic control
   of the agent's behaviors and decisions. Interpreting others' actions
   requires many elements of social cognition, including more complex
   functions like theory of mind. Although there is some consistency in
   the neural substrates identified as supporting these functions
   (e.g. the medial-prefrontal cortex and temporal-parietal junction,
   TPJ), most tasks employ static or sequential interactions that do not
   allow investigation of differing levels of social-cognitive engagement
   on a short time scale. In this study, we adapted a dynamic,
   competitive game modeled on a simplified penalty kick, wherein humans
   ($N=29$) controlled a ball, attempting to score against a human- or
   computer-controlled goalie, while measures of brain activation were
   obtained using functional magnetic resonance imaging (fMRI). This
   allowed us to quantitatively characterize subjects' strategic
   interactions with their opponent -- and how these strategies change
   over short and longer time scales. K-means clustering on the
   difference between the y position of the subject-controlled ball and
   human-controlled goalie revealed that participant behavior reduced to
   two clusters, one pair representing early feints and misdirections
   (i.e., separation between participant and opponent near the start of
   the trial) and the other representing strategies intended to hide the
   kicker's intention for as long as possible (i.e., separation only at
   the end of the trial). These clusters -- each further separable into
   pairs of upward or downward movements -- correspond to distinct
   behavioral strategies. Play against a computer opponent served as a
   non-social baseline with similar strategic interactions. Using
   multivariate pattern analysis (MVPA), we examined whether local brain
   regions carried information that predicted trial features (e.g.,
   opponent, outcome, strategy). Confirmatory analyses found that visual
   regions can be used to classify human from computer opponents at the
   time the opponent is revealed. MVPA of strategic interaction focused
   on the neural response differences between the two strategy-defined
   clusters. Using this dynamic task allows distinction of strategic
   elements of deception (e.g., timing of movement) from the conditions
   that elicit deception, the nature of the opponent, and the outcome of
   the action. The use of dynamic tasks facilitates the study of
   strategic social interaction on short timescales, opening avenues for
   causal studies of the supporting neural substrates.

* Poster notes

  Want to start with some sort of intro to deceptive behavior, mention
  that deceptive behavior is typically online and dynamic whereas lots
  of studies are static, talk a bit about the monkey study, then move
  into our stuff.

  From McKell, in reference to commit [[https://github.com/billbrod/SfN.2015/commit/18c63e0c7d7f1125d357a273b8b2d8351fdba04f][18c63e0]]: titles are a bit long
  and should focus on what should be taken away, not details. Can add
  details in bullets below. Someone should be able to just read the
  titles and understand what I want them to take away. For this, write
  my main question and what answer I want them to leave with (should
  be how or why not yes/no). Would be good to have preliminary data
  from JF or Caroline in that first column: why is this task good,
  what does it tell us that others don't? What does human data bring
  to the equation? What are our goals? Then can shrink those two big
  screen pics and move to the second column (have movie example of
  trial on tablet). Describe the planned imaging analyses and can
  maybe add a few things last minute. Put references in bottom right
  corner.

  - [X] Write main question and answer down, use as frame
  - [X] Rework titles
  - [X] Add bullet points
  - [X] Get preliminary data from JF or Caroline in first column
  - [X] Why this task? What does human data bring? What are our goals?
  - [X] Shrink and move screen pics to second column
  - [ ] Put movie example on tablet
  - [X] Describe imaging analysis
    - [X] Create image summarizing MVPA searchlight regression (see
      McKell's poster for inspiration, use [[https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html?js=1#svm-toy-js][LibSVM]] applet to generate
      example regression)
    - [X] Create image summarizing Stelzer et al's cluster method (see
      their figures 1 and 2, but those are very text heavy
  - [X] Check if players using mixed-strategy outperform those who use
    only one -- doesn't quite look like that, sent note to McKell to
    see what he thinks


  Main question: How do humans use deception to attempt to beat their
  opponents in a competitive, dynamic game?

  Answer: They use a mixture of two strategies, early and late
  deception, in an attempt to both deceive and be unpredictable. 

  To make this more interesting, looking at trial-to-trial
  dependencies (correlation on metric between trials and metric over
  time) would help, as well as showing there's no difference in
  win/loss percentages. Could also include figure showing metric
  vs. computer; include distribution?

  For monkey stuff: all their figures are either mutual information
  (which we can't use) or neuronal recordings. What are their points?
  That monkeys much less predictable against live opponents, use
  pre-planned feints (neurons in DLPFC) to attempt to deceive
  opponents. Inhibiting those neurons makes them much more
  predictable. Additionally, eye gaze prior to trial onset much more
  predictable than position until very close to end. So this shows us
  that there are neuronal correlates of planned deception.

  Why this task? Dynamic, allows us to investigate deception online,
  more like real-world situation where agents are reacting to each
  other, in contrast to many other tasks where deception is static or
  sequential. 

  What does human data bring? Humans appear to use more deception, in
  more intentional ways than other primates (?), our data shows that
  the manner in which they deceive is very different (go back much
  more often, movements are not nearly as pre-planned)

  What are our goals? To see how similar humans are to monkeys, how
  participants use deception to attempt to beat their opponents in a
  competitive game.

* Spiel

  Competition requires a player to deceive or somehow out-maneuever
  their opponent in order to achieve their goal, a process that is
  inherently responsive and flexible. But most tasks used to study
  deception or competition are static, with a turn-taking structure
  that limits interaction between players. We used a simplified
  penalty shot task, originally developed for macaque
  electrophysiology studies, to investigate dynamic deception as it
  happens in real time.

  Monkeys playing this task showed pre-planned deception: the eyes and
  DLPFC neuron activity of the monkey controlling the ball at the
  beginning of the trail were predictive of the ball's location at the
  end of the trial (more predictive than location alone until towards
  the end of the trial). This pattern was not seen when playing
  against a algorithmically-controlled goalie or a goalie that was
  simply a recording of earlier trials, which suggests that it was
  specifically the engagement with the other monkey that drove this. 

  To see if humans showed a similar pattern, we gathered fMRI data
  from 29 participants while they controlled the ball in a penalty
  shot task. They needed to get past a human goalie to score (also had
  data against a computer goalie, but their actions there aren't as
  interesting, so I'm not discussing them here). They had 10 seconds
  to finish the game, or they would time out. They could move forward
  or backward, up or down, in any way they wanted.

  As an attempt to see what they were doing, we took the y-difference
  between the ball and the goalie, normalized the length of the trial
  to 80 (average is 3.44 seconds) and used k-means clustering (4
  means) to see what patterns existed in the data. We saw two
  strategies, each expressed in up and down directions: "early" and
  "late". 

  The early trials are trials where the participant got separation
  early and then doubled back when attempting to score. This seems
  like misdirection. The late trials are those where the participant
  was tracked by the goalie the whole way and then made a break
  towards the end. This seems like hiding of information.
  
  This is clearly not all the structure in the data, but it is some of
  it, so we decided to run with it.

  We used PCA to reduce the dimensionality of the data and found,
  remarkably, that the clustering and categorization was pretty much
  the same all the way down to 2 dimensions. When we plot the trials
  in 2 dimensions though, we notice that the trials seem to vary
  continuously between the extremes corresponding to these
  archetypical strategies. 

  Therefore, we decided that classification between these strategies
  doesn't make too much sense, since it seems to be pretty
  continuous. Instead, we defined a strategy metric, which summarizes
  where a trial lies between these two extremes. And this is the
  target we use for whole brain MVPA searchlight regression.

  So MVPA searchlight regression is similar to your classic MVPA
  searchlight classification, where you take a 3-voxel sphere centered
  at each voxel in the brain, look at the activation within that
  sphere, and see how that relates to your target. For classification,
  you fit a classifier, like a linear SVM, to separate your
  categories. For regression, we use Nu-parameterized support vector
  regression to determine how the activity of the voxels in the sphere
  relate to the trial-wise metric. This gets you an error value (for
  us 1 - correlation between real and predicted metric) in each
  searchlight across the brain. We did this within each subject
  separately, using between-run cross-validation (4 runs for all but
  one subject, who had 3).

  To determine statistical significance, we used Stelzer et al 2013's
  group cluster thresholding method. It was originally developed for
  MVPA classification, but works for regression as well. In it, you
  permute the labels (metric values) on all trials 50 times such that
  all subjects, searchlights and cv folds have the same permutation
  (to preserve any correlations there). You then run your MVPA on
  these permuted maps so that, for each subject, you have 50 chance
  accuracy maps (takes a while). You then boot-strap up a group null
  distribution of accuracy maps by picking one map from each subject
  (with replacement) and averaging them together, 100k times. You then
  use these group chance maps to construct voxel-by-voxel null
  distributions (to compare to real group accuracy map), thresholding
  at p=.005. The key step is the next one though: we're not interested
  in solitary voxels, we're interested in coherent clusters, so you
  also construct a null distribution of clusters, constructing
  clusters in your chance maps (considering voxels within a cluster
  together if their faces tough) and using them to create a null
  distribution of cluster sizes. You then use this to threshold
  cluster sizes at p=.05, and then correct for multiple comparisons
  corrections (again at p=.05) and see what remains.

  We're not totally finished with this yet, but we have some
  preliminary imaging results. All of the above process reveals three
  clusters significantly predictive of the strategy metric: right
  ventral temporal cortex, right inferior parietal cortex, and right
  occipital cortex.

  Our next steps: run some other data through this pipeline to validate
  it, look into the metric more (we know that this is still hiding
  some variability, want to see how important/big that remaining
  variability is).

  So, conclusion: subject behavior has two broad archetypes, and our
  strategy metric allows us to summarize their strategy on a
  trial-by-trial basis. Preliminary imaging results as above.

  _If people ask_:

  About threshold of $p=.005$: Stelzer et al 2013 recommends a
  threshold between .005 and .001, we went with .005 because at .001
  we're concerned that clusters are getting broken up. That's still a
  concern here, actually, and we're looking to change the "connected
  scheme" from face- to edge- or vertex-connected to get them back
  together
